{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80651eab-5e3f-4d9f-841d-a34d6254fe4d",
   "metadata": {},
   "source": [
    "# Statistics 101\n",
    "In this notebook, we will discuss some basic concepts in statistics that you need for your assignment.\n",
    "\n",
    "You already know the concept of a mean, being the sum of the items divided by the number of items.\n",
    "\n",
    "we are using numpy for this. numpy is a library that can do array manipulation in a very fast way. All the things you will do later is based on numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06669d9-530b-4761-9767-ce88dafaf113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.array([1,3,7,5,3,8,9])\n",
    "length = X.shape[0]\n",
    "print(X.sum()/length)\n",
    "print(X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86161e6a-9596-42cb-8057-a96ee12cfd10",
   "metadata": {},
   "source": [
    "As you see, you can calculate the mean by taking sum and divide it by the number of elements in the array. You can also directly take the mean by using `X.mean()` \n",
    "\n",
    "The mean itself does not say that much. if you have a set of numbers, uniformly ranging from 0 to 1000, the mean (or average) will be around 500, but this average does not say that much about the individual values. For that reason, we try to capture the spread of set of numbers. We can go into the details of normal distributions and Gauss, but we won't do that for now. We just state how we measure this spread: We take the difference from each individual cases with the calculated mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c9d9e5-5ce2-49e0-82f5-2b5f2e5bc4b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences = X - X.mean()\n",
    "print(differences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce27092-74ab-4127-b45e-d5384afe5c28",
   "metadata": {},
   "source": [
    "if we would add up these values, they would end up at zero (or a very very small value):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae228e1-4028-4969-8a4e-bca21d46341d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(differences.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2e1f2-a4d4-499d-aee4-1fda0f244361",
   "metadata": {},
   "source": [
    "if we take the sum of squares of these differences, the signs disapear in the individual values and we get something of a total spread: the sum of squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851d9863-8fa9-43ef-ab88-ea4bfd0bd46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "differences_squared = differences ** 2\n",
    "print(differences_squared)\n",
    "print(f\"Sum of squares: {differences_squared.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a066d-f171-4544-9965-339ee45f7388",
   "metadata": {},
   "source": [
    "This value is actually not the one we like, since it grows with the number of items, so we divide it by the number of items, giving us the so called *variance*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe50d59f-d78a-4333-8628-f13e49c916b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variance = differences_squared.sum() / differences_squared.shape[0]\n",
    "print(f\"variance: {variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec75d21d-9947-4ea3-bfb9-4b04fd2a57fe",
   "metadata": {},
   "source": [
    "We can calculate the variance of a numpy array with `X.var()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dd5f05-d066-4fd4-8897-5dfb67c87db9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"variance: {X.var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a86b6f-36e1-4b95-8776-df8fa04ad3b1",
   "metadata": {},
   "source": [
    "The *standard deviation* is the square root of the variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ab7c1-5dde-4f0f-a19b-736aa72be798",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"{X.std()} = {np.sqrt(X.var())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be4c04d-a81a-4ee4-8b56-217116d7b723",
   "metadata": {},
   "source": [
    "This number tells us something about how sure we are about an estimate, like the mean. There is a theory about it (the one of Gauss with his normal distribution), which we are not going to explain now. I will, for now explain it by an example. Suppose we take the daily temperatures in the summer for the region of south limburg (maastricht). For this, open the file `temps.csv`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b808a5b1-0850-4483-af4a-58a80cf5e1c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open(\"temps.csv\", \"rt\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    temps = np.array(list(reader)).astype(float)\n",
    "print(temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c34dfe-7cac-46fb-80e2-63d217c4357c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The first column (column 0 in python) is the day number in July, the second is the average temperature, the third is the maximum temperature and the last is the minimum temperature. We can take the daily average temperature by selecting the second column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d225c14c-e4d8-43fc-9f84-b41dc59e81e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = temps[:,1]\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b36048e-6a83-4873-8899-fa307a407479",
   "metadata": {},
   "source": [
    "## Assignment 1.\n",
    "Please calculate the average, variance and standard deviation of this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8acaef-126c-4000-81ba-c16c0bec1073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc05e6f-d8b8-42e1-9263-c75063933407",
   "metadata": {},
   "source": [
    "As you see from the series, the temperatures fluctuate and are most of the time under 20 degrees. For a statistician it could be interesting to know if the temperature is *significantly* lower than 20 degrees. in that case, it is not only enough to look at the average, but also the standard deviation. Rule of thumb within statistics (with a 95% certainty) is to create an area of about two standard deviations below and two standard deviations about the mean and see if the 20 degrees falls within this interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239ac82d-a118-4ce5-aaae-5efc254d6a90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "interval = (T.mean() - 2*T.std(), T.mean()+2*T.std())\n",
    "print(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad2f59-8ddb-4766-9d3d-88d67d435ef2",
   "metadata": {},
   "source": [
    "as you can see, the interval goes from 13.6 degrees to 22.6 degrees. the temperature does not deviate significantly from 20 degrees. All the temperatures in this range are reasonable temperatures during this period. Question is: what does the average say about the temperature?\n",
    "\n",
    "We call this the confidence interval, and it introduces an uncertainty on the value we are measuring. The role of statistics is to see what we still can claim about the temperature in july around maastricht.\n",
    "\n",
    "Scientific findings often only have their validity with respect to this error and that is why it is important to look at them. Even everything you read about whatever statistics is valid under some assumptions, where the confidence interval is one of. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d094b2-f1a7-479f-9f99-271087133d20",
   "metadata": {},
   "source": [
    "# Population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f718f21e-00b1-4268-b6cd-45cc7f6e2382",
   "metadata": {},
   "source": [
    "Another important concept we talk about is population. What are we making a statistics about? all dutch people? all dutch inhabitants? All dutch companies? note that the word *dutch* already restricts the population. As Statistics Netherlands, we make statistics about something that is Dutch. Not because we are nationalistic, but because we have to deliver information to the other parts of the government so they can make policies. \n",
    "\n",
    "So the concept population is important. If we say that the average length of a dutch male is 1.84 and of a dutch female is 1.70, this means that we have to estimate this length as accurate as possible for all individuals in the dutch population: this statement should be as true as possible.\n",
    "\n",
    "So, how do we do that? First, let us generate a dutch population. The lengths have a standard deviation of about 7 cm, so we can create a population based on this information. You don't have to understand this completely, but here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd94a5e-d548-4e64-966c-96fafad75016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numpy.random import normal, shuffle, choice\n",
    "avgmale = 184\n",
    "sdmale = 7\n",
    "avgfemale = 170\n",
    "sdfemale = 7\n",
    "Nmale = 8745468\n",
    "Nfemale = 8845004\n",
    "\n",
    "males = normal(avgmale, sdmale, Nmale)\n",
    "print(f\"males: {males.mean()} +/- {males.std()}\")\n",
    "females = normal(avgfemale, sdfemale, Nfemale)\n",
    "print(f\"females: {females.mean()} +/- {females.std()}\")\n",
    "population = np.concatenate([males,females])\n",
    "shuffle(population)\n",
    "print(f\"The average length of dutch people is {population.mean()} cm, with a standard deviation of {population.std()}\")\n",
    "\n",
    "#for the future, we save this population\n",
    "males = np.concatenate([males, np.zeros(males.shape)]).reshape(2,Nmale).transpose()\n",
    "females = np.concatenate([females, np.ones(females.shape)]).reshape(2,Nfemale).transpose()\n",
    "dataset = np.concatenate([males,females])\n",
    "shuffle(dataset)\n",
    "import csv\n",
    "with open(\"lengths.csv\", \"wt\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9149507f-683d-4965-b981-46b659bce4f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "You see that the total standard deviation is larger than the standard deviation per gender. Why is that?\n",
    "\n",
    "Now we will determine the average length of a dutch person by what we call a simple random sample. We take a sample of 100 people and determine on the basis of them what the average is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50345fe-d3ef-408c-a06f-24b1e08ce033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample = choice(population, replace=False, size=100)\n",
    "print(sample.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496e0afa-f9e8-4e46-bf5b-db02a8734530",
   "metadata": {},
   "source": [
    "you see that, everytime you execute the previous cell, another value occurs, but it is always around that 177cm that we found in the population. \n",
    "\n",
    "This fluctuation depends on the number of samples that we take. The more samples we take, the less the value fluctuates. In the next cell, a simple script is written that takes the average of 100 repetitions of this experiment and calculates the variance. note that we use the variable `sample_size` to determine how many samples we take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6bfd51-88cb-4fe9-9c1d-3605550533cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "estimated_lengths = []\n",
    "for rep in range(100):\n",
    "    sample = choice(population, replace=False, size=sample_size)\n",
    "    estimated_lengths.append(sample.mean())\n",
    "print(f\"variance with a sample size of 100 is equal to: {np.array(estimated_lengths).var()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03867b25-b30e-4513-a36a-80c0929c6f3d",
   "metadata": {},
   "source": [
    "With list comprehension, we can write this shorter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d072e-ec38-49de-8ec9-a5897b19aab2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_size = 100\n",
    "estimated_lengths = np.array([choice(population, replace=False,size=sample_size).mean() for _ in range(100)])\n",
    "variance = estimated_lengths.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9f6ddb-b003-43ae-8809-fdaf3b91e33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now we want to do this for different sample sizes. We will not do that for many sample sizes but for some. this script takes a while, so start it and take a break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81467b9-581e-41bb-bdb9-ae68aedf2e98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "variances = []\n",
    "SAMPLE_SIZES = [50,100,200,400,800,1600]\n",
    "for sample_size in tqdm(SAMPLE_SIZES):\n",
    "    estimated_lengths = np.array([choice(population, replace=False,size=sample_size).mean() for _ in range(100)])\n",
    "    variances.append(estimated_lengths.var())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6a7a9-e94c-4289-a83f-5b9f9decab4e",
   "metadata": {},
   "source": [
    "Now we plot the results and see something really interesting: The bigger the sample size is, the smaller the variance is between the individual estimates. And further: Already with 800 samples, the value is quite stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e5a2d-8043-4ca0-a108-5a4a5de425d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "variances = np.array(variances)\n",
    "sample_sizes = np.array(SAMPLE_SIZES)\n",
    "plt.plot(sample_sizes, variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0db27f-c24e-419c-9a6f-6520cf77d7be",
   "metadata": {},
   "source": [
    "This is in a nutshell what we do when we send out questionaires: we optimize the number of samples in such a way that we can say, with a certain confidence, that, for instance, the average length of a dutch citizen is X cm. \n",
    "\n",
    "The double loop, programmed above is exactly what is going to happen in the next notebook, where we try to determine the confidence of the estimate of a certain variable based on the amount of data we put inside a machine learning algorithm. Have fun!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc4170e-2329-404e-8594-bfbbc8991f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
